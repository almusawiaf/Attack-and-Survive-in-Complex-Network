{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0e62d30",
   "metadata": {},
   "source": [
    "# Percolation analysis\n",
    "\n",
    "* read a network\n",
    "* drop the links\n",
    "* add links based on some measure (for example, link weight)\n",
    "\n",
    "* we measure on scale (0-1) how quickly they make a one complete component\n",
    "* Percolation = |N_LCC|/|N|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f35cbec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standard networks dataset\\dolphins\\dolphins.gml\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'standard networks dataset\\\\dolphins\\\\dolphins.gml'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 73\u001b[0m\n\u001b[1;32m     71\u001b[0m networks \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m net \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(datasets)):\n\u001b[0;32m---> 73\u001b[0m     g \u001b[38;5;241m=\u001b[39m \u001b[43mread_graph2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m g:\n\u001b[1;32m     75\u001b[0m         networks[datasets[net]] \u001b[38;5;241m=\u001b[39m g\n",
      "Cell \u001b[0;32mIn[1], line 53\u001b[0m, in \u001b[0;36mread_graph2\u001b[0;34m(g)\u001b[0m\n\u001b[1;32m     51\u001b[0m     G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mread_adjlist(file_name, create_using \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mGraph(), nodetype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ext\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gml\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m---> 53\u001b[0m     G \u001b[38;5;241m=\u001b[39m \u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_gml\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m ext\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mtx\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     55\u001b[0m     G \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/networkx/utils/decorators.py:845\u001b[0m, in \u001b[0;36margmap.__call__.<locals>.func\u001b[0;34m(_argmap__wrapper, *args, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(\u001b[38;5;241m*\u001b[39margs, __wrapper\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 845\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43margmap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_lazy_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43m__wrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m<class 'networkx.utils.decorators.argmap'> compilation 5:3\u001b[0m, in \u001b[0;36margmap_read_gml_1\u001b[0;34m(path, label, destringizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mbz2\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgzip\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01minspect\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/networkx/utils/decorators.py:191\u001b[0m, in \u001b[0;36mopen_file.<locals>._open_file\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    188\u001b[0m     \u001b[38;5;66;03m# could be None, or a file handle, in which case the algorithm will deal with it\u001b[39;00m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m path, \u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 191\u001b[0m fobj \u001b[38;5;241m=\u001b[39m \u001b[43m_dispatch_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fobj, \u001b[38;5;28;01mlambda\u001b[39;00m: fobj\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'standard networks dataset\\\\dolphins\\\\dolphins.gml'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import networkx as nx\n",
    "import community\n",
    "from networkx.algorithms.community import greedy_modularity_communities, girvan_newman\n",
    "import os\n",
    "from DA import pls_da1\n",
    "datasets = [\"\\dolphins\\dolphins.gml\",\n",
    "            \"\\polbooks\\out2.txt\",\n",
    "            \"\\word_adjacencies.gml\\word_adjacencies.gml\",\n",
    "            \"\\\\arenas-email\\\\out2.txt\",\n",
    "            \"Karate\",\n",
    "            \"Erdos Renyi\",\n",
    "            \"\\\\USAir97\\\\USAir97.mtx\", \n",
    "            \"\\\\circuits\\s208_st.txt\",\n",
    "            \"\\\\circuits\\s420_st.txt\",\n",
    "            \"\\\\circuits\\s838_st.txt\",\n",
    "            \"\\\\E. Coli\\E. Coli.txt\",\n",
    "            \"Barabasi_albert_graph\",\n",
    "            \"\\\\facebook\\\\0.edges\",\n",
    "            \"\\\\facebook\\\\107.edges\",\n",
    "            \"\\\\facebook\\\\348.edges\",\n",
    "            \"\\\\facebook\\\\414.edges\",\n",
    "            \"\\\\facebook\\\\686.edges\",\n",
    "            \"\\\\facebook\\\\1684.edges\",\n",
    "            \"\\\\bio-celegans\\\\bio-celegans.mtx\",\n",
    "            \"\\\\bn-macaque-rhesus_brain_2\\\\bn-macaque-rhesus_brain_2.txt\",\n",
    "            '\\\\soc-tribes\\\\soc-tribes.txt',\n",
    "            '\\\\fb-pages-food\\\\fb-pages-food.txt',\n",
    "            '\\\\bn-cat-mixed-species_brain_1\\\\bn-cat-mixed-species_brain_1.txt',\n",
    "            '\\\\ca-sandi_auths\\\\ca-sandi_auths.mtx',\n",
    "            '\\\\soc-firm-hi-tech\\\\soc-firm-hi-tech.txt']\n",
    "\n",
    "def read_graph2(g):\n",
    "    file_name = 'standard networks dataset' + datasets[int(g)]\n",
    "    print(file_name)\n",
    "    G = nx.Graph()\n",
    "    if g==4:\n",
    "        G = nx.karate_club_graph()\n",
    "    elif g==5:\n",
    "        # nodes = int(input(\"enter number of nodes?\"))\n",
    "        # edges= int(input(\"enter number of edges?\"))\n",
    "        G = nx.gnm_random_graph(500, 1500)\n",
    "    elif g==11:\n",
    "        # nodes = int(input(\"enter number of nodes?\"))\n",
    "        # edges= int(input(\"enter number of edges?\"))\n",
    "        # p = int(input(\"enter P value?\"))\n",
    "        G = nx.barabasi_albert_graph(500, 3)\n",
    "    else:\n",
    "        ext = os.path.splitext(file_name)[1]\n",
    "        if ext=='.edges':\n",
    "            G = nx.read_adjlist(file_name, create_using = nx.Graph(), nodetype = int)\n",
    "        elif ext=='.gml':\n",
    "            G = nx.read_gml(file_name)\n",
    "        elif ext=='.mtx':\n",
    "            G = None\n",
    "            #matrix = scipy.io.mmread(file_name)\n",
    "            #G = nx.from_scipy_sparse_matrix(matrix)\n",
    "        elif ext=='.txt':\n",
    "            file = open(file_name, 'r')\n",
    "            lines=  file.readlines()\n",
    "            G = nx.Graph()\n",
    "            for line in lines:\n",
    "                if \" \" in line:\n",
    "                    N = line.split(\" \")\n",
    "                else:\n",
    "                    N = line.split(\"\\t\")\n",
    "                G.add_edge(N[0], N[1])\n",
    "    return G\n",
    "\n",
    "# read the networks\n",
    "networks = {}\n",
    "for net in range(len(datasets)):\n",
    "    g = read_graph2(net)\n",
    "    if g:\n",
    "        networks[datasets[net]] = g\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7c19fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "name = ['dolphins',\n",
    " 'polbooks',\n",
    " 'word_adjacencies',\n",
    " 'arenas-email',\n",
    " 'Karate',\n",
    " 'Erdos Renyi',\n",
    " 'circuits s208_st',\n",
    " 'circuits s420_st',\n",
    " 'circuits s838_st',\n",
    " 'E. Coli',\n",
    " 'Barabasi_albert_graph',\n",
    " 'facebook0',\n",
    " 'facebook107',\n",
    " 'facebook348',\n",
    " 'facebook414',\n",
    " 'facebook686',\n",
    " 'facebook1684',\n",
    " 'bn-macaque-rhesus_brain_2',\n",
    " 'soc-tribes',\n",
    " 'fb-pages-food',\n",
    " 'bn-cat-mixed-species_brain_1',\n",
    " 'soc-firm-hi-tech']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b45704",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [len(networks[n].nodes()) for n in networks]\n",
    "edges = [len(networks[n].edges()) for n in networks]\n",
    "\n",
    "pd.DataFrame({'$|N|$': nodes, '$|E|$': edges})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8f8999",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_edges(G, C):\n",
    "    '''return a weighted edges'''\n",
    "    W = []\n",
    "    for u,v in G.edges():\n",
    "        W.append([u, v, C[u]*C[v]])\n",
    "    return sorted(W, key=lambda x: x[2])\n",
    "\n",
    "def batch_list(lst):\n",
    "    \"\"\"\n",
    "    Divide a list into batches of an equal number of items (as close to 50 as possible).\n",
    "    \"\"\"\n",
    "    batch_size = (len(lst) + 49) // 50  # Calculate the batch size\n",
    "    num_batches = (len(lst) + batch_size - 1) // batch_size\n",
    "    batches = [lst[i*batch_size:(i+1)*batch_size] for i in range(num_batches)]\n",
    "    return batches\n",
    "\n",
    "def simulation(centr):\n",
    "    results = []\n",
    "    for network in networks.keys():\n",
    "        print(network)\n",
    "        G0 = networks[network]\n",
    "        bc_G0 = centr(G0)\n",
    "        W = weighted_edges(G0, bc_G0)\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(G0.nodes())\n",
    "        batches = batch_list(W)\n",
    "\n",
    "        result = []\n",
    "        for b in range(len(batches)):\n",
    "            for u,v,_ in batches[b]:\n",
    "                G.add_edge(u, v)\n",
    "            largest_component = G.subgraph(max(nx.connected_components(G), key=len))\n",
    "            result.append([b, len(largest_component)/len(G)])\n",
    "        results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317038c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(results, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    fig = plt.figure(dpi=600)\n",
    "    fig, ax = plt.subplots()\n",
    "    markers = ['+', 'x', 'o', 's', 'd', 'D', '*'] # Add your desired markers here\n",
    "    for d in range(len(results)):\n",
    "        data = results[d]\n",
    "        x = [item[0] for item in data]\n",
    "        y = [item[1] for item in data]\n",
    "        marker_idx = d % len(markers) # Choose marker based on index of the result\n",
    "        ax.plot(x, y, marker=markers[marker_idx], linewidth=0.5, markersize=3, label=name[d])\n",
    "\n",
    "    ax.set_xlabel('edges')\n",
    "    ax.set_ylabel(r'$|N_{LCC}| / |N| $')\n",
    "    ax.set_title(f'{title}-based weighted edges')\n",
    "    plt.legend()\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee9c96e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "centralities = [nx.degree_centrality, nx.betweenness_centrality, nx.closeness_centrality, nx.clustering]\n",
    "centr        = ['Degree'            ,   'Betweenness'          ,   'Closeness'          ,   'Clustering']\n",
    "sims = {}\n",
    "for i in range(4):\n",
    "    cent = centr[i]\n",
    "    sims[cent] = simulation(centralities[i])\n",
    "    plot(sims[cent], cent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2a55c",
   "metadata": {},
   "source": [
    "# Predicting robustness of networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f8ab8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = {}\n",
    "R['Networks'] = name\n",
    "for c in centr:\n",
    "    Rs = []\n",
    "    for i in range(22):\n",
    "        S = sims[c][i]\n",
    "        T, V = 0, 0\n",
    "        for t, v in S:\n",
    "            if v > V:\n",
    "                T = t\n",
    "                V = v\n",
    "        Rs.append(T)\n",
    "    R[c] = Rs\n",
    "pd.DataFrame(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9541e3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sim():\n",
    "    results = []\n",
    "    for network in networks.keys():\n",
    "        G0 = networks[network]\n",
    "        W = batch_list(list(G0.edges()))\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(G0.nodes())\n",
    "        batches = batch_list(W)\n",
    "\n",
    "        result = []\n",
    "        for b in range(len(batches)):\n",
    "            for (u,v) in batches[b][0]:\n",
    "                G.add_edge(u, v)\n",
    "            largest_component = G.subgraph(max(nx.connected_components(G), key=len))\n",
    "            result.append([b, len(largest_component)/len(G)])\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "plot(random_sim(), 'Random')\n",
    "sims_rnd = random_sim()\n",
    "\n",
    "Rs = []\n",
    "for i in range(22):\n",
    "    S = sims_rnd[i]\n",
    "    T, V = 0, 0\n",
    "    for t, v in S:\n",
    "        if v > V:\n",
    "            T = t\n",
    "            V = v\n",
    "    Rs.append(T)\n",
    "R['Random'] = Rs\n",
    "pd.DataFrame(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4334149",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation2():\n",
    "    '''Implementing reverse preferential attachment'''\n",
    "    results = []\n",
    "    k = 0.01\n",
    "    for network in networks.keys():\n",
    "        print(network)\n",
    "        G0 = networks[network]\n",
    "        d = nx.degree_centrality(G0)\n",
    "        W = [[u, v, (1/((d[u]+k)*(d[v]+k)))] for u,v in G0.edges()]\n",
    "        W = sorted(W, key=lambda x: x[2])\n",
    "        G = nx.Graph()\n",
    "        G.add_nodes_from(G0.nodes())\n",
    "        batches = batch_list(W)\n",
    "\n",
    "        result = []\n",
    "        for b in range(len(batches)):\n",
    "            for u,v,_ in batches[b]:\n",
    "                G.add_edge(u, v)\n",
    "            largest_component = G.subgraph(max(nx.connected_components(G), key=len))\n",
    "            result.append([b, len(largest_component)/len(G)])\n",
    "        results.append(result)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e16e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(simulation2(), 'Inverted PA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb08a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "sims_rnd = simulation2()\n",
    "\n",
    "Rs = []\n",
    "for i in range(22):\n",
    "    S = sims_rnd[i]\n",
    "    T, V = 0, 0\n",
    "    for t, v in S:\n",
    "        if v > V:\n",
    "            T = t\n",
    "            V = v\n",
    "    Rs.append(T)\n",
    "R['Inverted PA'] = Rs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2908a11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def properties(G):\n",
    "    \n",
    "    GCC = nx.transitivity(G)\n",
    "    ACC = nx.average_clustering(G)\n",
    "    d = nx.density(G)\n",
    "    r = nx.degree_assortativity_coefficient(G)    \n",
    "    lcg = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "    LCG = G.subgraph(lcg[0])    \n",
    "    ASP = nx.average_shortest_path_length(LCG)\n",
    "    diam = nx.diameter(LCG)\n",
    "\n",
    "    communities = greedy_modularity_communities(G)\n",
    "    mod = nx.community.modularity(G, communities)\n",
    "    \n",
    "    return  GCC, ACC, d, r, ASP, diam, mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805f9ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_properties = [properties(networks[g]) for g in networks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef944ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "GCCs = [i for i, _,_,_,_,_,_ in network_properties]\n",
    "ACCs = [i for _, i,_,_,_,_,_ in network_properties]\n",
    "ds   = [i for _, _,i,_,_,_,_ in network_properties]\n",
    "rs   = [i for _, _,_,i,_,_,_ in network_properties]\n",
    "ASPs = [i for _, _,_,_,i,_,_ in network_properties]\n",
    "diam = [i for _, _,_,_,_,i,_ in network_properties]\n",
    "comm = [i for _, _,_,_,_,_,i in network_properties]\n",
    "\n",
    "df2 = pd.DataFrame({'Networks': name, 'GCC': GCCs, 'ACC': ACCs, 'Density': ds, 'r': rs, 'ASP': ASPs, 'Diameter': diam}) \n",
    "df2.to_csv('Data/networks properties.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee6a5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('Data/networks properties.csv')\n",
    "print(df2)\n",
    "\n",
    "numeric_cols = df2.select_dtypes(include='number').columns\n",
    "df_quartiles = df2[numeric_cols].apply(lambda x: pd.qcut(x.dropna(), q=[0, 0.25, 0.5, 0.75, 1.0], labels=[0.25,0.5,0.75,1]) if x.dtype != object else x)\n",
    "df_quartiles['Networks'] = df2['Networks']\n",
    "df_quartiles['Mod']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9350140",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(R)\n",
    "\n",
    "medians = df1.median()\n",
    "print(medians)\n",
    "for column in df1.columns :\n",
    "    if column!= 'Networks':\n",
    "        median = medians[column]  # Retrieve the median for the column\n",
    "        df1[column] = (df1[column] < median).astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c7ea3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset = pd.merge(df_quartiles, df1, on='Networks')\n",
    "Dataset = Dataset.reindex(columns = ['Networks', 'GCC', 'Mod', 'Density', 'r', 'ASP', 'Diameter', 'Degree',\n",
    "       'Betweenness', 'Closeness', 'Clustering', 'Random', 'Inverted PA'])\n",
    "Dataset.to_csv('Data/vulnerability output.csv', index=False)\n",
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b890ec",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from scipy.signal import savgol_filter\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import KFold, cross_val_predict, train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "data = pd.read_csv('Data/velnerability output.csv')\n",
    "oldR = {}\n",
    "X = data.loc[:, ['GCC', 'Mod', 'Density', 'r', 'ASP', 'Diameter']]\n",
    "for c in [ 'Degree', 'Betweenness', 'Closeness', 'Clustering', 'Random', 'Inverted PA']:\n",
    "    Y = data[c]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=19)\n",
    "    oldR[c] = pls_da1(X_train, y_train, X_test).tolist()\n",
    "\n",
    "oldR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe75a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "R = {c: [i[0] for i in oldR[c]] for c in oldR}\n",
    "R['Measures'] = ['GCC', 'Mod', 'Density', 'r', 'ASP', 'Diameter']\n",
    "pd.DataFrame(R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abce153",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(R).to_csv('Data/final with mod.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
